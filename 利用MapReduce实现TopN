现在假设有这么一个需求，有一份数据是邮箱的访问信息，现在要算出访问次数位列前三的邮箱域名，案例数据如下：
wolys@21cn.com
zss1984@126.com
294522652@qq.com
simulateboy@163.com
zhoushigang_123@163.com
sirenxing424@126.com

Mapper类：
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MailTopNMapper extends Mapper<LongWritable,Text,Text,IntWritable>{
	public static final IntWritable one = new IntWritable(1);
	private String line = null;
	private int begin = 0;
	private int end = 0;
	private Text outkey = new Text();
	@Override
	public void map(LongWritable key,Text val,Context context) throws IOException, InterruptedException{
		line = val.toString();
		begin = line.indexOf("@");
		end = line.indexOf(".");
		if(end > begin){
			//把邮箱的服务器名解析出来
			outkey.set(line.substring(begin+1, end));
			context.write(outkey, one);
		}
	}
}

Reducer类：
import java.io.IOException;
import java.util.Comparator;
import java.util.Set;
import java.util.TreeSet;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MailTopNReducer extends Reducer<Text,IntWritable,Text,IntWritable>{
	//自定义一个内部类，按照邮箱的访问次数对数据在集合中进行排序
	TreeSet<String> set = new TreeSet<>(new Comparator<String>(){
		public int compare(String i1,String i2){
			String[] el1 = i1.split(",");
			String[] el2 = i2.split(",");
			return -(new Integer(Integer.parseInt(el1[0])).compareTo(Integer.parseInt(el2[0])));
		}
	});
	
	@Override
	public void reduce(Text key,Iterable<IntWritable> vals,Context context){
		int total = 0;
		
		for(IntWritable val : vals){
			total += val.get();
		}
		//将每个邮箱域名和次数放到相应的map中
		set.add(Integer.toString(total)+","+key.toString());
		
		//一旦set中的数量超过阀值，则剔除最后一组，这样就避免了内存溢出问题
		if(set.size() > 3){
			set.remove(set.last());
		}
		
	}
	/**
	 * 使用cleanup进行收尾工作
	 * @throws InterruptedException 
	 * @throws IOException 
	 */
	@Override
	public void cleanup(Context context) throws IOException, InterruptedException{
		for(String str : set){
			String[] eles = str.split(",");
			context.write(new Text(eles[1]), new IntWritable(Integer.parseInt(eles[0])));					
		}
	}
}

Main类：
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class MailTopNMain {
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);
		job.setJarByClass(MailTopNMain.class);
		
		job.setMapperClass(MailTopNMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		job.setReducerClass(MailTopNReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		FileInputFormat.setInputPaths(job, args[0]);
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.waitForCompletion(true);
		System.exit(0);
	}
}

在这里，有一点需要注意，在Reducer中，我们使用了TreeSet来收集数据，而不是使用Map，因为Map是根据访问次数键值来进行排序的，假设有两个域名访问次数相同，
则后面的value值会把之前的值替换掉，这样就不能体现实际的情况了。
